---
title: "Gesamtlösung Präsenz 4 Clustering"
output: html_document
---
```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```


## 1.0 Student*innen-Fragen

### 1.1 Bei der "dist()" methode, was genau ist der output? Verstehe, dass es die Distanzen der einzelnen Punkte zueinander sind und das diese dann für das Clustering genutzt werden, aber nicht was genau die row und column bedeuten

### 1.2 Dendrogram Aussage auf Foliensatz 13 etwas unklar

### 1.3 Wie interpretiert man den "silhouette" command?
![](silhouette.PNG)

Der Silhoette command in R gibt an, wie gut ein Punkt zu seinem Cluster passt im vergleich zum nächstgelegensten Cluster.
Der Output sind Werte von -1 bis 1. 
Im Bild oben ist:

* "o" der Datenpunkt den wir untersuchen
* "A" der Cluster in dem sich "o" momentan befindet
* "B" der Cluster der neben "o" am nächsten ist ("A" wird ignoriert)

Der Wert wird nun so berechnet, dass die Differenz des Abstands von (A,o) und (B,o) berechnet wird 
und dieser gewichtet wird mit dem maximalen Abstand von (A,o) und (B,o).

Dabei sprechen positive Werte dafür, dass der Punkt "o" gut zu seinem momentanigen Cluster passt
und negative Werte, dass er schlecht zu seinem momentanigen Cluster passt und wahrschinlich dem falschen Cluster zugeordnet ist. 
Ein neutraler Wert von 0 bedeutet, dass er eigentlich zu keinem der Cluster passt. Man spricht von einem
starken Zusammenhang (ob positiv oder negativ ist egal) ab einem Wert von über | 0.75 |.

Im Output von R kann man gut sehen, dass zumindest die ersten 10 Werte des Clusters 1 sehr
gut zum Cluster 1 passen im Vergleich zum nächsten Cluster 2. Man kann also davon ausgehen, dass
sich diese Punkte im richtigen Cluster befinden.
```{r}
library(cluster)
bike <- read.csv("bike.csv", stringsAsFactors = FALSE)
bike.dist <- dist(bike[,c("temp", "casual", "registered", "windspeed")])
bike.clust <- hclust(bike.dist)
sil <- silhouette(cutree(bike.clust, 3), dist = bike.dist)
head(sil, 10)
```

### 1.4 Aufgabe e) bitte zusammen anschauen und besprechen. 


# 2.0 Hausaufgaben

### 2.1 Beschreiben Sie in eigenen Worten, wie die Linkage-Varianten auf der Folie 10 jeweils funktionieren
![](cluster_links.PNG)

Der Clustering Prozess ist iterativ und in jedem Schritt werden die zwei Cluster fusioniert, 
welche die geringste Distanz zueinander aufweisen.
Diese Linkage-Verfahren bestimmen nun, wie die "Distanz" zwischen zwei Clustern definiert wird:

* Single Linkage (nächst gelegener Nachbar): Die Distanz zwischen zwei Clustern ist die Distanz der zwei nächst gelegenen Mitglieder.
Für die Definition der Distanz werden also einfach die zwei nächsten Punkte genommen.
Diese Methode hat einen entscheidenden Nachteil: Es kann schnell passieren, dass
zwei Cluster fusioniert werden, die eigentlich nur zwei zueinander nahe Punkte haben,
die restlichen Punkte aber weit voneinander entfernt sind. Dies nennt man dann
"Kettenform" und das wird im nächsten Abschnitt noch einmal erklärt.

* Sinlge Linkage (Kettenform): Nicht eine neue Linkage Methode. Zeigt aber genau das Problem auf, dass die Cluster eigentlich nicht zusammengehören aber aufgrund einer "kette" nun ein cluster sind. Der Cluster
ganz links und unten rechts gehören eigentlich nicht zusammen, aber durch den Punkt
zwischen den beiden werden sie trotzdem fusioniert.

* Complete Linkage (entferntester Nachbar): Die Distanz zwischen zwei Clustern ist die Distanz der zwei am weitesten entfernten Mitglieder. Dies verhindert das Problem der "Kettenbildung", führt aber ein anderes Problem ein. 
Ausreisser können dazu führen, dass Cluster fälschlicherweise fusioniert werden.

* Average Linkage: Die Distanz zwischen zwei Clustern ist der Durschnitt aller Distanzen von allen Punkten in den beiden Clustern.

* Centroid Linkage: Distanz zwischen Clustern ist die Distanz ihrer centroide (center of gravity or mean point). 
Dies ist in der Umsetzung dann relativ ähnlich zum "Average Linkage".

* Ward Linkage: Ziel ist es Cluster zu haben mit einer minimierten "within-cluser variance". Die Methode sucht also die zwei Cluster, die beim Fusionieren zur kleinsten Vergrösserung der "sum of squared estimate of errors (SSE)" führt.
Diese Methode ist besonders wertvoll, da man in der Statistik mit der Varianz oft sehr viel aussagen treffen kann.

In R kann man die Linkage Methode wie folgt auswählen. Der Standard ist "complete":
```{r}
iris.features <- iris[,1:4]
iris.dist = dist(iris.features, method="euclidean")
# Möglichkeiten sind: "ward.D", "ward.D2", "single", "complete", "average" (= UPGMA), "mcquitty" (= WPGMA), "median" (= WPGMC) or "centroid"
iris.hclust = hclust(iris.dist, method = "complete")
```

### 2.2 Führen Sie das hierarchische Clustering für den Iris-Datensatz durch und interpretieren Sie das Ergebnis anhand eines Dendogramms
Für diese Erklärung werde ich mich an [das Video aus den Folien](https://www.youtube.com/watch?v=q8G6iuZOWUw) halten und dieses erweitern.

Wir verwenden bekannten den Iris-Datensatz. Dieser ist für uns besonders praktisch, da
wir ja schon wissen welche Cluster eigentlich generiert werden sollten. Idealerweise
sollte unsere Clusteranalyse ein Resultat von 3 Cluster für die jeweiligen Spezies haben.

Als erstes schauen wir uns den Datensatz mal noch etwas an. Wir sehen, 
dass wir 4 stetige Variablen haben und einen Faktor (Spezies). Auch sehen wir, dass
die Werte von den einzelnen Columns ziemlich unterschiedlich sind in ihrer Grössenordnung. 
ZB sieht man Petal.Width hat einen Durchschnitt von 1.2 und Sepal.Length einen von 5.8.
Diese Unterschiede könnten unser Clustering verzerren, da die Unterschiedlichen
Grössenordnungen einen direkten Einfluss auf die Distanz haben. Deshalb werden wir unsere
Daten noch skalieren. Auch werden wir den Faktor Spezies rausfiltern, da man mit diesem
nicht eine Cluster-Analyse durchführen kann:
```{r}
summary(iris)

iris_filtered <- iris[,1:4]
iris_filtered <- scale(iris_filtered)

summary(iris_filtered)
```

Nun führen wir die Cluster-Analyse durch mit der Ward-Methode für das Linkage 
Verfahren und der euklidischen Distanz für die Distanzmessung. Anschliessend erstellen
wir das Dendrogram:
```{r}
# Hier berechnen wir die Distanzen der Datenpunkte
iris_filtered_distances <- dist(iris_filtered, method = "euclidean")
# Hier führen wir das eigentliche Clustering druch
iris.filtered_clustering <- hclust(iris_filtered_distances, method="ward.D2")
# Hier erstellen wir das Dendrogram. Hang und Cex sollen den Output schöner aussehen lassen, aber mit vielen Datenpunkten ist das leider schwer.
plot(iris.filtered_clustering, hang=-1, labels=iris$Species, cex=.7)
```

Das Dendrogram zeigt gut, dass es einen sehr schönen Cluster ganz links gibt. Dieser beinhaltet
nur Datenpunkte einer einzigen Spezies. Bei den anderen zwei Clustern ist die Auftrennung
leider nicht mehr so schön. Mit unserem Vorwissen, dass es 3 Spezies gibt, trennen wir 
trotzdem mal nach 3 Clustern auf und analysieren das Ergebnis

Man sieht bei der Analyse der Mittelwerte von Sepal.Length, dass die Cluster klare 
Unterschiede zueinander haben. Cluster 1 hat die tiefsten und 3 die höchsten Werte.

Wenn wir jetzt abgleichen mit der Spezies, sehen wir, dass der Cluster 1 fast nur 
aus Setosa besteht. Dieser Cluster ist also sehr gut gelungen. Auch Cluster zwei ist 
gut und besteht nur aus Versicolor. Cluster 3 scheint aber nicht so gut zu sein und hat
einen Mix aus Versicolor und Virginica.
```{r}
library(dplyr)
# Dendrogram bei 3 Clustern "schneiden"
groups.3 <- cutree(iris.filtered_clustering, 3)
# Sepal.Length anschauen
iris_with_clust <- cbind(iris, groups.3)
iris_with_clust %>% group_by(groups.3) %>%
  summarise(
    Mean = mean(Sepal.Length),
    Median = median(Sepal.Length),
    Min = min(Sepal.Length),
    Max = max(Sepal.Length)
  )
# Gruppen vergleichen
table(groups.3, iris$Species)
```

### 2.3 Führen Sie das kmeans-Clustering für den Iris-Datensatz durch

```{r}
# Lade den Iris-Datensatz
data(iris)

# Setze einen Seed für reproduzierbare Ergebnisse
set.seed(123) 

# Entferne die Spezies
iris_data <- iris[, -5]

# Führe K-Means-Clustering mit 3 Clustern durch
kmeans_result <- kmeans(iris_data, centers = 3)

# Füge die Cluster-Zuweisungen zum Iris-Datensatz hinzu
iris$cluster <- factor(kmeans_result$cluster)

# Erstelle einen Plot, der die Cluster visualisiert
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color=cluster)) +
  geom_point(alpha=0.7, size = 3) +
  labs(title = "K-Means Clustering des Iris-Datensatzes",
       x = "Petal Length",
       y = "Petal Width"
       ) +
  theme_minimal()
```

### 2.4 Was sind Vor- und Nachteile von kmeans gegenüber hclust

Vorteile:

* Gut für grosse Datenmengen geeignet, weil es weniger rechenintensiv ist.
* Das Verfahren ist einfach zu verstehen und kann einfach interpretiert werden.

Nachteile:

* Nach jeder Durchführung des K-Means Clustering können unterschledliche Cluster
entstehen.
* Man muss die Anzahl der Cluster im Vorhinein angeben, was nicht immer intuitiv oder leicht zu bestimmen ist.
* Die Ergebnisse können stark von der initialen Wahl der Clusterzentren abhängen.
* Ausreisser können das Ergebnis der Clusterbildung stark beeinflussen.

Ein Grundlegender Unterschied der zwei Modelle ist, dass Kmeans die Daten in die festgelegte Clusterzahl bringen muss. Hclust hingegen zeigt welche Cluster sich am ähnlichsten sind.

### 2.5 Zielfunktion bei k-Means hat man die untenstehende Formel mit der dazugehörigen Ableitung. 2  mit ∂L/(∂μ_1 )= ∑_(x_i∈S_j)▒2‖x_i-μ_j ‖ . Wie sieht das Gradient-Descent-Verfahren für das Updaten der Mittelwerte aus? Was bedeuten dabei x_i,  μ_j und S_j?

Die Kostenfunktion wird wie folgt definiert:

$$
L = \sum_{i=1}^{k} \sum_{\mathbf{x}_{j} \in S_{i}} ||\mathbf{x}_{j} - \boldsymbol{\mu}_{i}||^{2}
$$
$$
\frac{\partial L}{\partial \boldsymbol{\mu}_{1}} = \sum_{\mathbf{x}_{i} \in S_{j}} 2 ||\mathbf{x}_{j} -\boldsymbol{\mu}_{i}||
$$
  

# 3.0 Übungsaufgaben
Um den Bike Datensatz benutzen zu können, müssen wir ihn zuerst noch vorbereiten.
Dazu führe ich einfach den R-code aus dem Lösungsdokument 02 aus:

```{r}
library(dplyr)
library(stringr)

## inkosistenzen 
# das muss man eventuell anpassen
bike <- read.csv("bike.csv", stringsAsFactors = FALSE)
# alle buchstaben entfernen
bike <- bike %>%
  mutate(humidity = gsub('[a-z A-Z]','',humidity))

# casting
# auf richtigen datentyp casten
bike$humidity <- as.numeric(bike$humidity)
bike$holiday <- factor(bike$holiday, levels = c(0, 1), labels = c("no", "yes"))
bike$workingday <- factor(bike$workingday, levels = c(0, 1), labels = c("no", "yes"))
bike$season <- factor(bike$season, levels = c(1, 2, 3, 4), labels = c("spring",
                                                                      "summer", "fall", "winter"), ordered = TRUE )
bike$weather <- factor(bike$weather, levels = c(1,2,3,4), labels = c("clear","mist + cloudy", "light rain/snow", "heavy rain"))
bike$casual <- as.numeric(bike$casual)
bike$registered <- as.numeric(bike$registered)
bike$count <- as.numeric(bike$count)


# strings zu datum machen
library(lubridate)
bike <- bike %>%
  mutate(datetime = mdy_hm(datetime))

# adapt
# duplicates zusammenfC<hren mit string cleaning
unique(bike$sources)
bike$sources <- tolower(bike$sources)
bike$sources <- str_trim(bike$sources)
na_loc <- is.na(bike$sources)
bike$sources[na_loc] <- "unknown"

#Missing Values

bike <- na.omit(bike)

#Ausreisser 
outlier_values <- boxplot.stats(bike$windspeed)$out
bike[bike$windspeed %in% outlier_values, "windspeed"] <- NA
```

### 3.1 Führen Sie das hierarchische Clustering auf dem bike-Datensatz durch. Definieren Sie dazu, welche Variablen eingehen sollen. Interpretieren Sie das Ergebnis anhand eines Dendogramms

### 3.2 Führen Sie das kmeans-Clustering auf dem bike-Datensatz durch. Definieren Sie dazu, welche Variablen eingehen sollen. Interpretieren Sie das Ergebnis